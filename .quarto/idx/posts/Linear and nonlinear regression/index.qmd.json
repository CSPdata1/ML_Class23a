{"title":"Linear and nonlinear regression","markdown":{"yaml":{"title":"Linear and nonlinear regression","author":"Carlos Saint-Preux","date":"2023-12-10","categories":["news","code","analysis"],"image":"image.jpg","jupyter":"python3"},"headingText":"Understanding Linear and Nonlinear Regression: A Machine Learning Perspective","containsRefs":false,"markdown":"\n\n\n## Introduction\n\nRegression analysis is a fundamental technique in machine learning that aims to model the relationship between a dependent variable and one or more independent variables. Linear and nonlinear regression are two common approaches used to capture and predict these relationships. In this blog post, we will delve into the concepts of linear and nonlinear regression, provide code examples using a popular machine learning library, and present insightful data visualizations to enhance our understanding.\n\n![Linear plots with linear-ish results](images/IMG_0417.JPG)\n\n## Linear Regression\n\n### What is Linear Regression?\n\nLinear regression is a linear approach to modeling the relationship between a dependent variable (yy) and one or more independent variables (XX). The model assumes that the relationship can be represented by a linear equation:\n\ny=β0+β1X1+β2X2+...+βnXn+ϵy=β0​+β1​X1​+β2​X2​+...+βn​Xn​+ϵ\n\nHere, β0β0​ is the intercept, β1,β2,...,βnβ1​,β2​,...,βn​ are the coefficients, X1,X2,...,XnX1​,X2​,...,Xn​ are the independent variables, and ϵϵ is the error term.\n\n### Linear Regression in Python\n\nLet's implement a simple linear regression model using the popular scikit-learn library. We'll use the following code:\n\n```{python}\n# Importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Generating synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Making predictions on the test set\ny_pred = lin_reg.predict(X_test)\n\n```\n\n### Visualization 1: Scatter Plot with Regression Line\n\nNow, let's visualize the results by plotting a scatter plot of the data points along with the linear regression line:\n\n```{python}\n# Plotting the scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\n\n# Plotting the regression line\nplt.plot(X_test, y_pred, color='red', linewidth=3, label='Linear Regression Line')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph displays the actual data points in blue and the linear regression line in red, demonstrating how well the model fits the data.\n\n## Nonlinear Regression\n\n### What is Nonlinear Regression?\n\nWhile linear regression assumes a linear relationship, nonlinear regression allows for more complex, nonlinear relationships between the dependent and independent variables. The model can take various forms, such as quadratic, exponential, or logarithmic functions.\n\n### Nonlinear Regression in Python\n\nLet's consider a scenario where the relationship between XX and yy is quadratic. We can use scikit-learn's PolynomialFeatures to transform the features and apply linear regression:\n\n```{python}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Generating synthetic data with a quadratic relationship\ny_nonlinear = 1 + 2 * X + 0.5 * X**2 + np.random.randn(100, 1)\n\n# Transforming features to include quadratic term\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Splitting the data into training and testing sets\nX_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly, y_nonlinear, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model with quadratic features\nlin_reg_nonlinear = LinearRegression()\nlin_reg_nonlinear.fit(X_poly_train, y_poly_train)\n\n# Making predictions on the test set\ny_poly_pred = lin_reg_nonlinear.predict(X_poly_test)\n```\n\n### Visualization 2: Scatter Plot with Quadratic Regression Curve\n\nNow, let's visualize the results with a scatter plot and a quadratic regression curve:\n\n```{python}\n# Sorting X_test for better visualization\nX_test_sorted, y_poly_pred_sorted = zip(*sorted(zip(X_test, y_poly_pred)))\n\n# Plotting the scatter plot\nplt.scatter(X_test, y_poly_test, color='blue', label='Actual Data')\n\n# Plotting the quadratic regression curve\nplt.plot(X_test_sorted, y_poly_pred_sorted, color='green', linewidth=3, label='Quadratic Regression Curve')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Nonlinear Regression - Quadratic')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph showcases the actual data points in blue and the quadratic regression curve in green.\n\n### Visualization 3: Residuals Plot\n\nAnother insightful visualization is the residuals plot, which helps assess the performance of the model. Residuals are the differences between the actual and predicted values.\n\n```         \npython\n```\n\n```{python}\n# Calculating residuals for linear regression\nresiduals_linear = y_test - y_pred\n\n# Calculating residuals for quadratic regression\nresiduals_nonlinear = y_poly_test - y_poly_pred\n\n# Plotting residuals for linear regression\nplt.scatter(X_test, residuals_linear, color='red', label='Linear Regression Residuals')\n\n# Plotting residuals for quadratic regression\nplt.scatter(X_test, residuals_nonlinear, color='green', label='Quadratic Regression Residuals')\n\n# Adding a horizontal line at y=0\nplt.axhline(y=0, color='black', linestyle='--', linewidth=2)\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph illustrates the residuals for both linear and quadratic regression models. Ideally, residuals should be randomly distributed around the horizontal line at y=0y=0.\n\n### Visualization 4: Model Comparison\n\nLet's create a plot to compare the predictions of the linear and quadratic regression models:\n\n```{python}\n# Sorting X_test for better visualization\nX_test_sorted, y_pred_sorted = zip(*sorted(zip(X_test, y_pred)))\n\n# Plotting the scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\n\n# Plotting the linear regression line\nplt.plot(X_test_sorted, y_pred_sorted, color='red', linewidth=3, label='Linear Regression Line')\n\n# Sorting X_test for quadratic regression\nX_test_sorted, y_poly_pred_sorted = zip(*sorted(zip(X_test, y_poly_pred)))\n\n# Plotting the quadratic regression curve\nplt.plot(X_test_sorted, y_poly_pred_sorted, color='green', linewidth=3, label='Quadratic Regression Curve')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Model Comparison - Linear vs. Quadratic')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph provides a side-by-side comparison of the linear regression line and the quadratic regression curve.\n\n## Conclusion\n\nLinear and nonlinear regression are powerful tools in machine learning for modeling relationships between variables. By understanding these concepts and visualizing the results, we can gain valuable insights into the data and the performance of our models. The code examples and visualizations presented in this blog post serve as a starting point for exploring and implementing regression analysis in your machine learning projects.\n","srcMarkdownNoYaml":"\n\n# Understanding Linear and Nonlinear Regression: A Machine Learning Perspective\n\n## Introduction\n\nRegression analysis is a fundamental technique in machine learning that aims to model the relationship between a dependent variable and one or more independent variables. Linear and nonlinear regression are two common approaches used to capture and predict these relationships. In this blog post, we will delve into the concepts of linear and nonlinear regression, provide code examples using a popular machine learning library, and present insightful data visualizations to enhance our understanding.\n\n![Linear plots with linear-ish results](images/IMG_0417.JPG)\n\n## Linear Regression\n\n### What is Linear Regression?\n\nLinear regression is a linear approach to modeling the relationship between a dependent variable (yy) and one or more independent variables (XX). The model assumes that the relationship can be represented by a linear equation:\n\ny=β0+β1X1+β2X2+...+βnXn+ϵy=β0​+β1​X1​+β2​X2​+...+βn​Xn​+ϵ\n\nHere, β0β0​ is the intercept, β1,β2,...,βnβ1​,β2​,...,βn​ are the coefficients, X1,X2,...,XnX1​,X2​,...,Xn​ are the independent variables, and ϵϵ is the error term.\n\n### Linear Regression in Python\n\nLet's implement a simple linear regression model using the popular scikit-learn library. We'll use the following code:\n\n```{python}\n# Importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Generating synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Making predictions on the test set\ny_pred = lin_reg.predict(X_test)\n\n```\n\n### Visualization 1: Scatter Plot with Regression Line\n\nNow, let's visualize the results by plotting a scatter plot of the data points along with the linear regression line:\n\n```{python}\n# Plotting the scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\n\n# Plotting the regression line\nplt.plot(X_test, y_pred, color='red', linewidth=3, label='Linear Regression Line')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph displays the actual data points in blue and the linear regression line in red, demonstrating how well the model fits the data.\n\n## Nonlinear Regression\n\n### What is Nonlinear Regression?\n\nWhile linear regression assumes a linear relationship, nonlinear regression allows for more complex, nonlinear relationships between the dependent and independent variables. The model can take various forms, such as quadratic, exponential, or logarithmic functions.\n\n### Nonlinear Regression in Python\n\nLet's consider a scenario where the relationship between XX and yy is quadratic. We can use scikit-learn's PolynomialFeatures to transform the features and apply linear regression:\n\n```{python}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Generating synthetic data with a quadratic relationship\ny_nonlinear = 1 + 2 * X + 0.5 * X**2 + np.random.randn(100, 1)\n\n# Transforming features to include quadratic term\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Splitting the data into training and testing sets\nX_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly, y_nonlinear, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model with quadratic features\nlin_reg_nonlinear = LinearRegression()\nlin_reg_nonlinear.fit(X_poly_train, y_poly_train)\n\n# Making predictions on the test set\ny_poly_pred = lin_reg_nonlinear.predict(X_poly_test)\n```\n\n### Visualization 2: Scatter Plot with Quadratic Regression Curve\n\nNow, let's visualize the results with a scatter plot and a quadratic regression curve:\n\n```{python}\n# Sorting X_test for better visualization\nX_test_sorted, y_poly_pred_sorted = zip(*sorted(zip(X_test, y_poly_pred)))\n\n# Plotting the scatter plot\nplt.scatter(X_test, y_poly_test, color='blue', label='Actual Data')\n\n# Plotting the quadratic regression curve\nplt.plot(X_test_sorted, y_poly_pred_sorted, color='green', linewidth=3, label='Quadratic Regression Curve')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Nonlinear Regression - Quadratic')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph showcases the actual data points in blue and the quadratic regression curve in green.\n\n### Visualization 3: Residuals Plot\n\nAnother insightful visualization is the residuals plot, which helps assess the performance of the model. Residuals are the differences between the actual and predicted values.\n\n```         \npython\n```\n\n```{python}\n# Calculating residuals for linear regression\nresiduals_linear = y_test - y_pred\n\n# Calculating residuals for quadratic regression\nresiduals_nonlinear = y_poly_test - y_poly_pred\n\n# Plotting residuals for linear regression\nplt.scatter(X_test, residuals_linear, color='red', label='Linear Regression Residuals')\n\n# Plotting residuals for quadratic regression\nplt.scatter(X_test, residuals_nonlinear, color='green', label='Quadratic Regression Residuals')\n\n# Adding a horizontal line at y=0\nplt.axhline(y=0, color='black', linestyle='--', linewidth=2)\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph illustrates the residuals for both linear and quadratic regression models. Ideally, residuals should be randomly distributed around the horizontal line at y=0y=0.\n\n### Visualization 4: Model Comparison\n\nLet's create a plot to compare the predictions of the linear and quadratic regression models:\n\n```{python}\n# Sorting X_test for better visualization\nX_test_sorted, y_pred_sorted = zip(*sorted(zip(X_test, y_pred)))\n\n# Plotting the scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\n\n# Plotting the linear regression line\nplt.plot(X_test_sorted, y_pred_sorted, color='red', linewidth=3, label='Linear Regression Line')\n\n# Sorting X_test for quadratic regression\nX_test_sorted, y_poly_pred_sorted = zip(*sorted(zip(X_test, y_poly_pred)))\n\n# Plotting the quadratic regression curve\nplt.plot(X_test_sorted, y_poly_pred_sorted, color='green', linewidth=3, label='Quadratic Regression Curve')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Model Comparison - Linear vs. Quadratic')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n\n```\n\nThis graph provides a side-by-side comparison of the linear regression line and the quadratic regression curve.\n\n## Conclusion\n\nLinear and nonlinear regression are powerful tools in machine learning for modeling relationships between variables. By understanding these concepts and visualizing the results, we can gain valuable insights into the data and the performance of our models. The code examples and visualizations presented in this blog post serve as a starting point for exploring and implementing regression analysis in your machine learning projects.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Linear and nonlinear regression","author":"Carlos Saint-Preux","date":"2023-12-10","categories":["news","code","analysis"],"image":"image.jpg","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}