{
  "hash": "dc9f8e9fddaba4b0fe268841104c16fb",
  "result": {
    "markdown": "---\ntitle: Probability theory and random variables\nauthor: Carlos Saint-Preux\ndate: '2023-12-10'\ncategories:\n  - news\n  - code\n  - analysis\nimage: image.jpg\n---\n\nIntroduction:\n\nIn the vast landscape of machine learning, understanding the fundamentals is crucial for building robust models and making informed decisions. Probability theory and random variables form the backbone of many machine learning algorithms, providing a mathematical framework to model uncertainty and variability. In this blog, we'll delve into the core concepts of probability theory and random variables, and demonstrate their relevance through machine learning code and visualizations.\n\n### Probability Theory:\n\nProbability theory is a branch of mathematics that quantifies uncertainty. It provides a systematic way of reasoning about uncertain events and their likelihood. At the heart of probability theory are probabilities, which measure the likelihood of different outcomes. Let's start by exploring the basics of probability using Python code.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulating a fair six-sided die roll\noutcomes = np.arange(1, 7)\nprobabilities = np.ones(6) / 6\n\n# Visualizing the probability distribution\nplt.bar(outcomes, probabilities, color='skyblue')\nplt.title('Probability Distribution of a Fair Six-sided Die')\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=597 height=449}\n:::\n:::\n\n\nIn this example, we simulate a fair six-sided die roll and visualize the probability distribution. Each outcome has an equal probability of 1/6, demonstrating the fundamental concept of probability.\n\n### Random Variables:\n\nRandom variables are variables whose values are determined by chance. They can be discrete or continuous, representing outcomes of random events. Machine learning often deals with random variables, and understanding their properties is essential for designing effective models. Let's create a simple example using a continuous random variable.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Simulating a continuous random variable (e.g., normal distribution)\nmean = 0\nstd_dev = 1\nsamples = np.random.normal(mean, std_dev, 1000)\n\n# Visualizing the distribution\nplt.hist(samples, bins=30, density=True, color='lightcoral')\nplt.title('Normal Distribution - Random Variable Example')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=590 height=449}\n:::\n:::\n\n\nIIn this snippet, we generate samples from a normal distribution, a common random variable in statistics. The histogram provides a visual representation of the probability density function.\n\n### Machine Learning Application:\n\nProbability theory and random variables are integral to many machine learning algorithms. One common application is in Bayesian machine learning, where probability is used to model uncertainty in predictions. Let's consider a Bayesian classifier using the famous Iris dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import datasets\nfrom sklearn import metrics\n\n# Load Iris dataset\niris = datasets.load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n\n# Bayesian classifier\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Predictions\npredictions = model.predict(X_test)\n\n# Model accuracy\naccuracy = metrics.accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9777777777777777\n```\n:::\n:::\n\n\nIn this example, we apply a Gaussian Naive Bayes classifier to the Iris dataset. Bayesian methods leverage probability theory to update beliefs as new data is observed, making them particularly useful in scenarios with limited data.\n\n### Conclusion:\n\nProbability theory and random variables are foundational concepts that play a pivotal role in machine learning. They provide a rigorous framework for handling uncertainty, making informed decisions, and building reliable models. As demonstrated through code and visualizations, a solid understanding of these concepts is essential for any aspiring machine learning practitioner. Whether you're developing algorithms, making predictions, or interpreting results, probability theory and random variables will guide you through the intricate landscape of machine learning.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}