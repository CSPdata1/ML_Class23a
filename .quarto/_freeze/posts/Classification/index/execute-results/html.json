{
  "hash": "d2541167701c9758f62e1633fbbe5375",
  "result": {
    "markdown": "---\ntitle: Classification\nauthor: Carlos Saint-Preux\ndate: '2023-12-10'\ncategories:\n  - news\n  - code\n  - analysis\nimage: image.jpg\nr: rmarkdown\n---\n\n# Exploring Classification in Machine Learning: A Comprehensive Guide\n\n## Introduction:\n\nMachine learning has become an integral part of data analysis and decision-making processes. One of the fundamental tasks in machine learning is classification, where the goal is to assign predefined labels to input data based on its characteristics. In this blog post, we will delve into the world of classification, covering essential concepts, techniques, and providing hands-on code examples. Along the way, we'll use Python and popular machine learning libraries to implement classification algorithms and visualize the results.\n\n![](images/IMG_4253-01.jpeg)\n\n## Basics of Classification\n\n### What is Classification?\n\nClassification is a supervised learning technique where the algorithm learns from labeled training data and then predicts the class labels for unseen or future data points. The goal is to find a model that accurately maps the input features to the correct class.\n\n### Key Components\n\n1.  **Features:** These are the measurable properties or attributes of the data. In a classification problem, features are used to make predictions about the class labels.\n\n2.  **Labels/Classes:** These are the categories or groups that we want our model to predict. For instance, in a spam detection problem, the classes could be \"spam\" and \"non-spam.\"\n\n3.  **Training Data:** This is the labeled data used to train the classification model. It consists of input features and their corresponding class labels.\n\n4.  **Test Data:** Unlabeled data used to evaluate the performance of the trained model. The model predicts the class labels, and these predictions are compared with the actual labels to assess accuracy.\n\n## Classification Algorithms\n\n### 1. Understanding Classification Algorithms:\n\nClassification algorithms are supervised learning techniques that assign predefined labels to input data based on their characteristics. Common use cases include spam detection, sentiment analysis, and image recognition. Some popular classification algorithms include:\n\n1.  Logistic Regression\n\n2.  Decision Trees\n\n3.  Random Forest\n\n4.  Support Vector Machines (SVM)\n\n5.  k-Nearest Neighbors (k-NN)\n\n6.  Naive Bayes\n\nLet's talk a little bit more about the three of them.\n\n1.  **Logistic Regression:** Despite its name, logistic regression is used for binary classification problems. It models the probability of an instance belonging to a particular class.\n\n2.  **Decision Trees:** Decision trees recursively split the data based on features to create a tree-like structure. Each leaf node represents a class.\n\n<!-- -->\n\n3.  **Random Forests:** An ensemble method that builds multiple decision trees and combines their predictions. It often provides better generalization and robustness.\n\n### 2. Implementing Logistic Regression:\n\nLet's start with one of the foundational classification algorithms, Logistic Regression. This algorithm is well-suited for binary classification problems. Below is a simple R code snippet for implementing logistic regression using the `glm` function:\n\n\n```{r message=FALSE, warning=FALSE}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate sample data\nset.seed(123)\ndata <- data.frame(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = as.factor(sample(0:1, 100, replace = TRUE))\n)\n\n# Split data into training and testing sets\ntrain_indices <- sample(1:nrow(data), 0.7 * nrow(data))\ntrain_data <- data[train_indices, ]\ntest_data <- data[-train_indices, ]\n\n# Build logistic regression model\nmodel <- glm(y ~ x1 + x2, data = train_data, family = \"binomial\")\n\n# Make predictions on the test set\npredictions <- predict(model, newdata = test_data, type = \"response\")\n\n# Evaluate model performance\n# (metrics and confusion matrix code here)\n\n```\n\n\n### 3. Visualizing Model Performance:\n\nNow, let's visualize the performance of our logistic regression model using a ROC curve. We'll use the `pROC` package for this task:\n\n\n```{r message=FALSE, warning=FALSE}\n# Install and load the pROC package\n#install.packages(\"pROC\")\nlibrary(pROC)\n\n# Create ROC curve\nroc_curve <- roc(test_data$y, predictions)\nplot(roc_curve, main = \"ROC Curve\", col = \"blue\", lwd = 2)\n```\n\n\nThis code will generate a ROC curve that visually represents the trade-off between sensitivity and specificity.\n\n### 4. Exploring Decision Trees:\n\nDecision trees are powerful and interpretable classification algorithms. They recursively split the data based on features to create a tree-like structure. Below is an example of implementing a decision tree using the `rpart` package:\n\n\n```{r message=FALSE, warning=FALSE}\n# Install and load the rpart package\n#install.packages(\"rpart\")\nlibrary(rpart)\n\n# Build decision tree model\ntree_model <- rpart(y ~ x1 + x2, data = train_data, method = \"class\")\n\n# Visualize the decision tree\nplot(tree_model)\ntext(tree_model)\n```\n\n\n### 5. Visualizing Decision Boundaries:\n\nTo understand how our classification algorithm separates classes, we can visualize decision boundaries. Let's use the `ggplot2` package for this\n\n\n```{r message=FALSE, warning=FALSE}\n# Generate data for decision boundary plot\nlibrary(ggplot2)\nboundary_data <- expand.grid(\n  x1 = seq(min(data$x1), max(data$x1), length.out = 100),\n  x2 = seq(min(data$x2), max(data$x2), length.out = 100)\n)\n\n# Predict class labels for decision boundary data\nboundary_data$y_pred <- predict(tree_model, newdata = boundary_data, type = \"class\")\n\n# Plot decision boundaries\nggplot() +\n  geom_point(data = data, aes(x = x1, y = x2, color = y)) +\n  geom_tile(data = boundary_data, aes(x = x1, y = x2, fill = y_pred), alpha = 0.1) +\n  labs(title = \"Decision Boundaries\") +\n  theme_minimal()\n\n```\n\n\n## Data Visualization with Python 3\n\nVisualization is crucial for understanding the performance of a classification model. Let's create three essential visualizations: a confusion matrix, a ROC curve, and a precision-recall curve.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#setting up the stage \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn as sl \nfrom sklearn.linear_model import LinearRegression \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nx = np.arange(1, 25).reshape(12, 2)\ny = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\nx\ny\nx_train, x_test, y_train, y_test = train_test_split(x, y)\nx_train\nx_test\ny_train\ny_test\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n# Create and train the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n# Make predictions\ny_pred = model.predict(X_test)\n# Evaluate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.3333333333333333\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Example code for Confusion Matrix, ROC Curve, and Precision-Recall Curve\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n# Precision-Recall Curve\nprecision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1])\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='green', lw=2, label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='upper right')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=619 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=663 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-3.png){width=663 height=523}\n:::\n:::\n\n\n### 6. Conclusion:\n\nIn this blog post, we explored classification algorithms, focusing on logistic regression and decision trees. We implemented the algorithms in R using RStudio and visualized their performance and decision boundaries. Classification is a vast field with many algorithms, and this post serves as a starting point for your journey into the world of supervised learning. Experiment with different algorithms, datasets, and visualizations to deepen your understanding and hone your machine learning skills.\n\n![](images/347329-200.png)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}