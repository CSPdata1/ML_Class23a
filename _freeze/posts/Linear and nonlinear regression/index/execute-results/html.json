{
  "hash": "5818df265e23a6392a458efab2861947",
  "result": {
    "markdown": "---\ntitle: Linear and nonlinear regression\nauthor: Carlos Saint-Preux\ndate: '2023-12-10'\ncategories:\n  - news\n  - code\n  - analysis\nimage: image.jpg\n---\n\n# Understanding Linear and Nonlinear Regression: A Machine Learning Perspective\n\n## Introduction\n\nRegression analysis is a fundamental technique in machine learning that aims to model the relationship between a dependent variable and one or more independent variables. Linear and nonlinear regression are two common approaches used to capture and predict these relationships. In this blog post, we will delve into the concepts of linear and nonlinear regression, provide code examples using a popular machine learning library, and present insightful data visualizations to enhance our understanding.\n\n![Linear plots with linear-ish results](images/IMG_0417.JPG)\n\n## Linear Regression\n\n### What is Linear Regression?\n\nLinear regression is a linear approach to modeling the relationship between a dependent variable (yy) and one or more independent variables (XX). The model assumes that the relationship can be represented by a linear equation:\n\ny=β0+β1X1+β2X2+...+βnXn+ϵy=β0​+β1​X1​+β2​X2​+...+βn​Xn​+ϵ\n\nHere, β0β0​ is the intercept, β1,β2,...,βnβ1​,β2​,...,βn​ are the coefficients, X1,X2,...,XnX1​,X2​,...,Xn​ are the independent variables, and ϵϵ is the error term.\n\n### Linear Regression in Python\n\nLet's implement a simple linear regression model using the popular scikit-learn library. We'll use the following code:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Generating synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Making predictions on the test set\ny_pred = lin_reg.predict(X_test)\n```\n:::\n\n\n### Visualization 1: Scatter Plot with Regression Line\n\nNow, let's visualize the results by plotting a scatter plot of the data points along with the linear regression line:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Plotting the scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\n\n# Plotting the regression line\nplt.plot(X_test, y_pred, color='red', linewidth=3, label='Linear Regression Line')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=585 height=449}\n:::\n:::\n\n\nThis graph displays the actual data points in blue and the linear regression line in red, demonstrating how well the model fits the data.\n\n## Nonlinear Regression\n\n### What is Nonlinear Regression?\n\nWhile linear regression assumes a linear relationship, nonlinear regression allows for more complex, nonlinear relationships between the dependent and independent variables. The model can take various forms, such as quadratic, exponential, or logarithmic functions.\n\n### Nonlinear Regression in Python\n\nLet's consider a scenario where the relationship between XX and yy is quadratic. We can use scikit-learn's PolynomialFeatures to transform the features and apply linear regression:\n\n```         \npython\n```\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Generating synthetic data with a quadratic relationship\ny_nonlinear = 1 + 2 * X + 0.5 * X**2 + np.random.randn(100, 1)\n\n# Transforming features to include quadratic term\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Splitting the data into training and testing sets\nX_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly, y_nonlinear, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model with quadratic features\nlin_reg_nonlinear = LinearRegression()\nlin_reg_nonlinear.fit(X_poly_train, y_poly_train)\n\n# Making predictions on the test set\ny_poly_pred = lin_reg_nonlinear.predict(X_poly_test)\n```\n:::\n\n\n### Visualization 2: Scatter Plot with Quadratic Regression Curve\n\nNow, let's visualize the results with a scatter plot and a quadratic regression curve:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Sorting X_test for better visualization\nX_test_sorted, y_poly_pred_sorted = zip(*sorted(zip(X_test, y_poly_pred)))\n\n# Plotting the scatter plot\nplt.scatter(X_test, y_poly_test, color='blue', label='Actual Data')\n\n# Plotting the quadratic regression curve\nplt.plot(X_test_sorted, y_poly_pred_sorted, color='green', linewidth=3, label='Quadratic Regression Curve')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Nonlinear Regression - Quadratic')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=585 height=449}\n:::\n:::\n\n\nThis graph showcases the actual data points in blue and the quadratic regression curve in green.\n\n### Visualization 3: Residuals Plot\n\nAnother insightful visualization is the residuals plot, which helps assess the performance of the model. Residuals are the differences between the actual and predicted values.\n\n```         \npython\n```\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Calculating residuals for linear regression\nresiduals_linear = y_test - y_pred\n\n# Calculating residuals for quadratic regression\nresiduals_nonlinear = y_poly_test - y_poly_pred\n\n# Plotting residuals for linear regression\nplt.scatter(X_test, residuals_linear, color='red', label='Linear Regression Residuals')\n\n# Plotting residuals for quadratic regression\nplt.scatter(X_test, residuals_nonlinear, color='green', label='Quadratic Regression Residuals')\n\n# Adding a horizontal line at y=0\nplt.axhline(y=0, color='black', linestyle='--', linewidth=2)\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=587 height=449}\n:::\n:::\n\n\nThis graph illustrates the residuals for both linear and quadratic regression models. Ideally, residuals should be randomly distributed around the horizontal line at y=0y=0.\n\n### Visualization 4: Model Comparison\n\nLet's create a plot to compare the predictions of the linear and quadratic regression models:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Sorting X_test for better visualization\nX_test_sorted, y_pred_sorted = zip(*sorted(zip(X_test, y_pred)))\n\n# Plotting the scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\n\n# Plotting the linear regression line\nplt.plot(X_test_sorted, y_pred_sorted, color='red', linewidth=3, label='Linear Regression Line')\n\n# Sorting X_test for quadratic regression\nX_test_sorted, y_poly_pred_sorted = zip(*sorted(zip(X_test, y_poly_pred)))\n\n# Plotting the quadratic regression curve\nplt.plot(X_test_sorted, y_poly_pred_sorted, color='green', linewidth=3, label='Quadratic Regression Curve')\n\n# Adding labels and legend\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Model Comparison - Linear vs. Quadratic')\nplt.legend()\n\n# Displaying the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=585 height=449}\n:::\n:::\n\n\nThis graph provides a side-by-side comparison of the linear regression line and the quadratic regression curve.\n\n## Conclusion\n\nLinear and nonlinear regression are powerful tools in machine learning for modeling relationships between variables. By understanding these concepts and visualizing the results, we can gain valuable insights into the data and the performance of our models. The code examples and visualizations presented in this blog post serve as a starting point for exploring and implementing regression analysis in your machine learning projects.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}