{
  "hash": "23bbaeb627c0fd22e87203e3fd419bd0",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Carlos Saint-Preux\"\ndate: \"2023-12-10\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\nr: rmarkdown\n---\n\n\n# Exploring Classification in Machine Learning: A Comprehensive Guide\n\n## Introduction:\n\nMachine learning has become an integral part of data analysis and decision-making processes. One of the fundamental tasks in machine learning is classification, where the goal is to assign predefined labels to input data based on its characteristics. In this blog post, we will delve into the world of classification, covering essential concepts, techniques, and providing hands-on code examples. Along the way, we'll use Python and popular machine learning libraries to implement classification algorithms and visualize the results.\n\n![](images/IMG_4253-01.jpeg)\n\n## Basics of Classification\n\n### What is Classification?\n\nClassification is a supervised learning technique where the algorithm learns from labeled training data and then predicts the class labels for unseen or future data points. The goal is to find a model that accurately maps the input features to the correct class.\n\n### Key Components\n\n1.  **Features:** These are the measurable properties or attributes of the data. In a classification problem, features are used to make predictions about the class labels.\n\n2.  **Labels/Classes:** These are the categories or groups that we want our model to predict. For instance, in a spam detection problem, the classes could be \"spam\" and \"non-spam.\"\n\n3.  **Training Data:** This is the labeled data used to train the classification model. It consists of input features and their corresponding class labels.\n\n4.  **Test Data:** Unlabeled data used to evaluate the performance of the trained model. The model predicts the class labels, and these predictions are compared with the actual labels to assess accuracy.\n\n## Classification Algorithms\n\n### 1. Understanding Classification Algorithms:\n\nClassification algorithms are supervised learning techniques that assign predefined labels to input data based on their characteristics. Common use cases include spam detection, sentiment analysis, and image recognition. Some popular classification algorithms include:\n\n1.  Logistic Regression\n\n2.  Decision Trees\n\n3.  Random Forest\n\n4.  Support Vector Machines (SVM)\n\n5.  k-Nearest Neighbors (k-NN)\n\n6.  Naive Bayes\n\nLet's talk a little bit more about the three of them.\n\n1.  **Logistic Regression:** Despite its name, logistic regression is used for binary classification problems. It models the probability of an instance belonging to a particular class.\n\n2.  **Decision Trees:** Decision trees recursively split the data based on features to create a tree-like structure. Each leaf node represents a class.\n\n3.  **Random Forests:** An ensemble method that builds multiple decision trees and combines their predictions. It often provides better generalization and robustness.\n\n### 2. Implementing Logistic Regression:\n\nLet's start with one of the foundational classification algorithms, Logistic Regression. This algorithm is well-suited for binary classification problems. Below is a simple R code snippet for implementing logistic regression using the `glm` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate sample data\nset.seed(123)\ndata <- data.frame(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = as.factor(sample(0:1, 100, replace = TRUE))\n)\n\n# Split data into training and testing sets\ntrain_indices <- sample(1:nrow(data), 0.7 * nrow(data))\ntrain_data <- data[train_indices, ]\ntest_data <- data[-train_indices, ]\n\n# Build logistic regression model\nmodel <- glm(y ~ x1 + x2, data = train_data, family = \"binomial\")\n\n# Make predictions on the test set\npredictions <- predict(model, newdata = test_data, type = \"response\")\n\n# Evaluate model performance\n# (metrics and confusion matrix code here)\n```\n:::\n\n\n### 3. Visualizing Model Performance:\n\nNow, let's visualize the performance of our logistic regression model using a ROC curve. We'll use the `pROC` package for this task:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the pROC package\n#install.packages(\"pROC\")\nlibrary(pROC)\n\n# Create ROC curve\nroc_curve <- roc(test_data$y, predictions)\nplot(roc_curve, main = \"ROC Curve\", col = \"blue\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThis code will generate a ROC curve that visually represents the trade-off between sensitivity and specificity.\n\n### 4. Exploring Decision Trees:\n\nDecision trees are powerful and interpretable classification algorithms. They recursively split the data based on features to create a tree-like structure. Below is an example of implementing a decision tree using the `rpart` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the rpart package\n#install.packages(\"rpart\")\nlibrary(rpart)\n\n# Build decision tree model\ntree_model <- rpart(y ~ x1 + x2, data = train_data, method = \"class\")\n\n# Visualize the decision tree\nplot(tree_model)\ntext(tree_model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### 5. Visualizing Decision Boundaries:\n\nTo understand how our classification algorithm separates classes, we can visualize decision boundaries. Let's use the `ggplot2` package for this\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data for decision boundary plot\nlibrary(ggplot2)\nboundary_data <- expand.grid(\n  x1 = seq(min(data$x1), max(data$x1), length.out = 100),\n  x2 = seq(min(data$x2), max(data$x2), length.out = 100)\n)\n\n# Predict class labels for decision boundary data\nboundary_data$y_pred <- predict(tree_model, newdata = boundary_data, type = \"class\")\n\n# Plot decision boundaries\nggplot() +\n  geom_point(data = data, aes(x = x1, y = x2, color = y)) +\n  geom_tile(data = boundary_data, aes(x = x1, y = x2, fill = y_pred), alpha = 0.1) +\n  labs(title = \"Decision Boundaries\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\njupyter: python3\n\n## Data Visualization with Python 3\n\nVisualization is crucial for understanding the performance of a classification model. Let's create three essential visualizations: a confusion matrix, a ROC curve, and a precision-recall curve.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#setting up the stage \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn as sl \nfrom sklearn.linear_model import LinearRegression \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nx = np.arange(1, 25).reshape(12, 2)\ny = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[ 1,  2],\n       [ 3,  4],\n       [ 5,  6],\n       [ 7,  8],\n       [ 9, 10],\n       [11, 12],\n       [13, 14],\n       [15, 16],\n       [17, 18],\n       [19, 20],\n       [21, 22],\n       [23, 24]])\n```\n:::\n\n```{.python .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n```\n:::\n\n```{.python .cell-code}\nx_train, x_test, y_train, y_test = train_test_split(x, y)\nx_train\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[ 3,  4],\n       [ 1,  2],\n       [17, 18],\n       [19, 20],\n       [13, 14],\n       [11, 12],\n       [ 9, 10],\n       [21, 22],\n       [ 7,  8]])\n```\n:::\n\n```{.python .cell-code}\nx_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[23, 24],\n       [15, 16],\n       [ 5,  6]])\n```\n:::\n\n```{.python .cell-code}\ny_train\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([1, 0, 1, 0, 0, 0, 1, 1, 0])\n```\n:::\n\n```{.python .cell-code}\ny_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0, 1, 1])\n```\n:::\n\n```{.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n# Create and train the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\n# Make predictions\ny_pred = model.predict(X_test)\n# Evaluate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.3333333333333333\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Example code for Confusion Matrix, ROC Curve, and Precision-Recall Curve\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n\n```{.python .cell-code}\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Figure size 1600x1200 with 0 Axes>\n```\n:::\n\n```{.python .cell-code}\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[<matplotlib.lines.Line2D object at 0x137907310>]\n```\n:::\n\n```{.python .cell-code}\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[<matplotlib.lines.Line2D object at 0x1351fa350>]\n```\n:::\n\n```{.python .cell-code}\nplt.xlabel('False Positive Rate')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText(0.5, 0, 'False Positive Rate')\n```\n:::\n\n```{.python .cell-code}\nplt.ylabel('True Positive Rate')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText(0, 0.5, 'True Positive Rate')\n```\n:::\n\n```{.python .cell-code}\nplt.title('Receiver Operating Characteristic (ROC) Curve')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText(0.5, 1.0, 'Receiver Operating Characteristic (ROC) Curve')\n```\n:::\n\n```{.python .cell-code}\nplt.legend(loc='lower right')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<matplotlib.legend.Legend object at 0x1378f5a10>\n```\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=768}\n:::\n\n```{.python .cell-code}\n# Precision-Recall Curve\nprecision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1])\nplt.figure(figsize=(8, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Figure size 1600x1200 with 0 Axes>\n```\n:::\n\n```{.python .cell-code}\nplt.plot(recall, precision, color='green', lw=2, label='Precision-Recall curve')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[<matplotlib.lines.Line2D object at 0x1379bb290>]\n```\n:::\n\n```{.python .cell-code}\nplt.xlabel('Recall')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText(0.5, 0, 'Recall')\n```\n:::\n\n```{.python .cell-code}\nplt.ylabel('Precision')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText(0, 0.5, 'Precision')\n```\n:::\n\n```{.python .cell-code}\nplt.title('Precision-Recall Curve')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText(0.5, 1.0, 'Precision-Recall Curve')\n```\n:::\n\n```{.python .cell-code}\nplt.legend(loc='upper right')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<matplotlib.legend.Legend object at 0x137a03cd0>\n```\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=768}\n:::\n:::\n\n\n### 6. Conclusion:\n\nIn this blog post, we explored classification algorithms, focusing on logistic regression and decision trees. We implemented the algorithms in R using RStudio and visualized their performance and decision boundaries. Classification is a vast field with many algorithms, and this post serves as a starting point for your journey into the world of supervised learning. Experiment with different algorithms, datasets, and visualizations to deepen your understanding and hone your machine learning skills.\n\n![](images/347329-200.png)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}