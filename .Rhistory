# Importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
# Generating synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Creating and training the linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
# Making predictions on the test set
y_pred = lin_reg.predict(X_test)
# Plotting the scatter plot
plt.scatter(X_test, y_test, color='blue', label='Actual Data')
# Plotting the regression line
plt.plot(X_test, y_pred, color='red', linewidth=3, label='Linear Regression Line')
# Adding labels and legend
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.legend()
# Displaying the plot
plt.show()
# Example code for Random Forests
from sklearn.ensemble import RandomForestClassifier
# Create and train the model
model = RandomForestClassifier()
model.fit(X_train, y_train)
# Make predictions
y_pred = model.predict(X_test)
# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(f"Accuracy: {accuracy}")
pip3 install scikit-learn
# Logistic Regression Example
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# Load Iris dataset
# ...
# Split data into training and testing sets
# ...
# Initialize and train the model
model = LogisticRegression()
model.fit(X_train, y_train)
# Make predictions
predictions = model.predict(X_test)
# Evaluate accuracy
accuracy = accuracy_score(y_test, predictions)
print(f"Logistic Regression Accuracy: {accuracy}")
# Decision Tree Example
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
# Load Titanic dataset
# ...
# Initialize and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)
# Visualize the decision tree
plt.figure(figsize=(10, 6))
tree.plot_tree(model, feature_names=feature_names, class_names=class_names, filled=True)
plt.show()
reticulate::repl_python()
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Generate sample data
set.seed(123)
data <- data.frame(
x1 = rnorm(100),
x2 = rnorm(100),
y = as.factor(sample(0:1, 100, replace = TRUE))
)
# Split data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Build logistic regression model
model <- glm(y ~ x1 + x2, data = train_data, family = "binomial")
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")
# Evaluate model performance
# (metrics and confusion matrix code here)
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Generate sample data
set.seed(123)
data <- data.frame(
x1 = rnorm(100),
x2 = rnorm(100),
y = as.factor(sample(0:1, 100, replace = TRUE))
)
# Split data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Build logistic regression model
model <- glm(y ~ x1 + x2, data = train_data, family = "binomial")
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")
# Evaluate model performance
# (metrics and confusion matrix code here)
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Generate sample data
set.seed(123)
data <- data.frame(
x1 = rnorm(100),
x2 = rnorm(100),
y = as.factor(sample(0:1, 100, replace = TRUE))
)
# Split data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Build logistic regression model
model <- glm(y ~ x1 + x2, data = train_data, family = "binomial")
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")
# Evaluate model performance
# (metrics and confusion matrix code here)
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Generate sample data
set.seed(123)
data <- data.frame(
x1 = rnorm(100),
x2 = rnorm(100),
y = as.factor(sample(0:1, 100, replace = TRUE))
)
# Split data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Build logistic regression model
model <- glm(y ~ x1 + x2, data = train_data, family = "binomial")
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")
# Evaluate model performance
# (metrics and confusion matrix code here)
# Install and load the pROC package
install.packages("pROC")
library(pROC)
# Create ROC curve
roc_curve <- roc(test_data$y, predictions)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
# Generate data for decision boundary plot
boundary_data <- expand.grid(
x1 = seq(min(data$x1), max(data$x1), length.out = 100),
x2 = seq(min(data$x2), max(data$x2), length.out = 100)
)
# Predict class labels for decision boundary data
boundary_data$y_pred <- predict(tree_model, newdata = boundary_data, type = "class")
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Generate sample data
set.seed(123)
data <- data.frame(
x1 = rnorm(100),
x2 = rnorm(100),
y = as.factor(sample(0:1, 100, replace = TRUE))
)
# Split data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Build logistic regression model
model <- glm(y ~ x1 + x2, data = train_data, family = "binomial")
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")
# Evaluate model performance
# (metrics and confusion matrix code here)
# Install and load the pROC package
install.packages("pROC")
library(pROC)
# Create ROC curve
roc_curve <- roc(test_data$y, predictions)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
install.packages("pROC")
# Install and load the rpart package
install.packages("rpart")
library(rpart)
# Build decision tree model
tree_model <- rpart(y ~ x1 + x2, data = train_data, method = "class")
# Visualize the decision tree
plot(tree_model)
text(tree_model)
# Generate data for decision boundary plot
boundary_data <- expand.grid(
x1 = seq(min(data$x1), max(data$x1), length.out = 100),
x2 = seq(min(data$x2), max(data$x2), length.out = 100)
)
# Predict class labels for decision boundary data
boundary_data$y_pred <- predict(tree_model, newdata = boundary_data, type = "class")
# Plot decision boundaries
ggplot() +
geom_point(data = data, aes(x = x1, y = x2, color = y)) +
geom_tile(data = boundary_data, aes(x = x1, y = x2, fill = y_pred), alpha = 0.1) +
labs(title = "Decision Boundaries") +
theme_minimal()
# Generate data for decision boundary plot
library(ggplot2)
boundary_data <- expand.grid(
x1 = seq(min(data$x1), max(data$x1), length.out = 100),
x2 = seq(min(data$x2), max(data$x2), length.out = 100)
)
# Predict class labels for decision boundary data
boundary_data$y_pred <- predict(tree_model, newdata = boundary_data, type = "class")
# Plot decision boundaries
ggplot() +
geom_point(data = data, aes(x = x1, y = x2, color = y)) +
geom_tile(data = boundary_data, aes(x = x1, y = x2, fill = y_pred), alpha = 0.1) +
labs(title = "Decision Boundaries") +
theme_minimal()
library(rmarkdown)
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Generate sample data
set.seed(123)
data <- data.frame(
x1 = rnorm(100),
x2 = rnorm(100),
y = as.factor(sample(0:1, 100, replace = TRUE))
)
# Split data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Build logistic regression model
model <- glm(y ~ x1 + x2, data = train_data, family = "binomial")
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")
# Evaluate model performance
# (metrics and confusion matrix code here)
